{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":5407,"databundleVersionId":868283,"sourceType":"competition"}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pandas\n!pip install numpy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T12:28:48.284068Z","iopub.execute_input":"2025-03-17T12:28:48.284368Z","iopub.status.idle":"2025-03-17T12:28:55.879394Z","shell.execute_reply.started":"2025-03-17T12:28:48.284338Z","shell.execute_reply":"2025-03-17T12:28:55.878210Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T12:28:55.880775Z","iopub.execute_input":"2025-03-17T12:28:55.881064Z","iopub.status.idle":"2025-03-17T12:28:58.127201Z","shell.execute_reply.started":"2025-03-17T12:28:55.881035Z","shell.execute_reply":"2025-03-17T12:28:58.126476Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 1. Importing Dataset and Overview","metadata":{}},{"cell_type":"markdown","source":"We have to make relevant changes in our test data set too hence we have to import both and simulatenously change both accordingly","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/train.csv\")\ndf_test = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/test.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T12:28:58.128075Z","iopub.execute_input":"2025-03-17T12:28:58.128378Z","iopub.status.idle":"2025-03-17T12:28:58.185898Z","shell.execute_reply.started":"2025-03-17T12:28:58.128351Z","shell.execute_reply":"2025-03-17T12:28:58.185133Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 1.1 Overview of the data\n","metadata":{}},{"cell_type":"code","source":"df_train.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T12:28:58.187782Z","iopub.execute_input":"2025-03-17T12:28:58.188009Z","iopub.status.idle":"2025-03-17T12:28:58.210432Z","shell.execute_reply.started":"2025-03-17T12:28:58.187985Z","shell.execute_reply":"2025-03-17T12:28:58.209777Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(df_train.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T12:28:58.211323Z","iopub.execute_input":"2025-03-17T12:28:58.211605Z","iopub.status.idle":"2025-03-17T12:28:58.215090Z","shell.execute_reply.started":"2025-03-17T12:28:58.211578Z","shell.execute_reply":"2025-03-17T12:28:58.214415Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Structure of the data. Columns and their data types","metadata":{}},{"cell_type":"code","source":"df_train.info()","metadata":{"trusted":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2025-03-17T12:28:58.216051Z","iopub.execute_input":"2025-03-17T12:28:58.216331Z","iopub.status.idle":"2025-03-17T12:28:58.235840Z","shell.execute_reply.started":"2025-03-17T12:28:58.216302Z","shell.execute_reply":"2025-03-17T12:28:58.235089Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T12:28:58.236984Z","iopub.execute_input":"2025-03-17T12:28:58.237260Z","iopub.status.idle":"2025-03-17T12:28:58.250083Z","shell.execute_reply.started":"2025-03-17T12:28:58.237236Z","shell.execute_reply":"2025-03-17T12:28:58.249426Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Understanding the response data ","metadata":{}},{"cell_type":"markdown","source":"Summary statistics of response data","metadata":{}},{"cell_type":"code","source":"df_train[\"SalePrice\"].describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T12:28:58.250986Z","iopub.execute_input":"2025-03-17T12:28:58.251235Z","iopub.status.idle":"2025-03-17T12:28:58.258659Z","shell.execute_reply.started":"2025-03-17T12:28:58.251210Z","shell.execute_reply":"2025-03-17T12:28:58.258026Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Visualizing the response data","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\ndf_train[\"SalePrice\"].plot(kind = \"hist\", bins = 20, xlabel = \"Sale Price\", color = \"red\", edgecolor = 'black')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T12:28:58.259487Z","iopub.execute_input":"2025-03-17T12:28:58.259732Z","iopub.status.idle":"2025-03-17T12:28:59.599220Z","shell.execute_reply.started":"2025-03-17T12:28:58.259706Z","shell.execute_reply":"2025-03-17T12:28:59.598581Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Performing log transformation to response data , so that skewness doesnt affect our results.","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Check the skewness of SalePrice\nsaleprice_skewness = df_train['SalePrice'].skew()\nprint(f\"Skewness of SalePrice: {saleprice_skewness:.2f}\")\n\n# Plot original SalePrice distribution\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nsns.histplot(df_train['SalePrice'], bins=30, kde= True)\nplt.title(\"Original SalePrice Distribution\")\n\n# Apply log transformation only if the skewness is high (> 1)\nif saleprice_skewness > 1:\n    df_train['SalePrice'] = np.log1p(df_train['SalePrice'])  # log1p(x) = log(x + 1)\n    print(\"Log transformation applied to SalePrice.\")\nelse:\n    print(\"No transformation applied. SalePrice is not highly skewed.\")\n\n# Plot transformed SalePrice distribution\nplt.subplot(1, 2, 2)\nsns.histplot(df_train['SalePrice'], bins=30, kde=True , color = 'red')\nplt.title(\"Transformed SalePrice (if applied)\")\n\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T12:28:59.600086Z","iopub.execute_input":"2025-03-17T12:28:59.600403Z","iopub.status.idle":"2025-03-17T12:29:01.598953Z","shell.execute_reply.started":"2025-03-17T12:28:59.600377Z","shell.execute_reply":"2025-03-17T12:29:01.598262Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 1.2 Missing Values","metadata":{}},{"cell_type":"markdown","source":"Finding columns with missing values and their data type","metadata":{}},{"cell_type":"code","source":"missing_percentage = df_train.isnull().mean() * 100 #finds percentage missing in a single column and stores as series \n\nmissing_percentage = missing_percentage.sort_values(ascending = False ) #sorts missing percentage in descending order\n\ncolumn_data_types = df_train.dtypes[missing_percentage.index] #gathers the data types of columns with missing values\n\n# Combine missing percentage and data types into a DataFrame for display\nmissing_percentage_with_types = pd.DataFrame({\n    'Missing Percentage': missing_percentage,\n    'Data Type': column_data_types\n})\n\nprint(missing_percentage_with_types[missing_percentage > 0]) #display columns with missing data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T12:29:01.599908Z","iopub.execute_input":"2025-03-17T12:29:01.600272Z","iopub.status.idle":"2025-03-17T12:29:01.611955Z","shell.execute_reply.started":"2025-03-17T12:29:01.600237Z","shell.execute_reply":"2025-03-17T12:29:01.611250Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate missing data percentage and data types for the test set\nmissing_percentage_test = df_test.isnull().mean() * 100  # finds percentage missing in each column and stores as a series\n\n# Sort missing percentage in descending order\nmissing_percentage_test = missing_percentage_test.sort_values(ascending=False)\n\n# Gather the data types of columns with missing values in the test set\ncolumn_data_types_test = df_test.dtypes[missing_percentage_test.index]\n\n# Combine missing percentage and data types into a DataFrame for display\nmissing_percentage_with_types_test = pd.DataFrame({\n    'Missing Percentage': missing_percentage_test,\n    'Data Type': column_data_types_test\n})\n\n# Display columns with missing data in the test set\nprint(missing_percentage_with_types_test[missing_percentage_test > 0])  # show columns with missing data\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T12:29:01.615074Z","iopub.execute_input":"2025-03-17T12:29:01.615355Z","iopub.status.idle":"2025-03-17T12:29:01.631402Z","shell.execute_reply.started":"2025-03-17T12:29:01.615325Z","shell.execute_reply":"2025-03-17T12:29:01.630662Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2. Data Pre-processing","metadata":{}},{"cell_type":"markdown","source":"## 2.1 Removing columns with high percentage of missing values.( >50% missing)","metadata":{}},{"cell_type":"markdown","source":"These columns have too much missing data. They will no longer be useful in our model hence we drop them","metadata":{}},{"cell_type":"code","source":"columns_to_drop = missing_percentage[missing_percentage  > 50].index #gathers index with missing percentage > 50\n\nprint(f\"Columns dropped: {', '.join(columns_to_drop)}\") #printing which columns have been dropped\n\ndf_train = df_train.drop(columns = columns_to_drop) #droping those columns from train set\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T12:29:01.632307Z","iopub.execute_input":"2025-03-17T12:29:01.632582Z","iopub.status.idle":"2025-03-17T12:29:01.642882Z","shell.execute_reply.started":"2025-03-17T12:29:01.632554Z","shell.execute_reply":"2025-03-17T12:29:01.642211Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Drop those columns from the test set\ndf_test = df_test.drop(columns=columns_to_drop)  # Dropping the same columns from the test set\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T12:29:01.643702Z","iopub.execute_input":"2025-03-17T12:29:01.643935Z","iopub.status.idle":"2025-03-17T12:29:01.652941Z","shell.execute_reply.started":"2025-03-17T12:29:01.643911Z","shell.execute_reply":"2025-03-17T12:29:01.652333Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Columns dropped: PoolQC, MiscFeature, Alley, Fence, MasVnrType","metadata":{}},{"cell_type":"markdown","source":"## 2.2 Find categorical columns and change their *Dtype* from `object` to `Categorical`","metadata":{}},{"cell_type":"markdown","source":"This allows for efficent storage and computation","metadata":{}},{"cell_type":"code","source":"categorical_columns = df_train.select_dtypes(include=['object']).columns #getting column names with dtypes that are object \n\nfor col in categorical_columns:\n    df_train[col] = df_train[col].astype('category') #changinf their types to category","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T12:29:01.653803Z","iopub.execute_input":"2025-03-17T12:29:01.654057Z","iopub.status.idle":"2025-03-17T12:29:01.684674Z","shell.execute_reply.started":"2025-03-17T12:29:01.654032Z","shell.execute_reply":"2025-03-17T12:29:01.684060Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get column names with dtype 'object' from the test set\ncategorical_columns_test = df_test.select_dtypes(include=['object']).columns\n\n# Change the type of these columns to 'category'\nfor col in categorical_columns_test:\n    df_test[col] = df_test[col].astype('category')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T12:29:01.685571Z","iopub.execute_input":"2025-03-17T12:29:01.685852Z","iopub.status.idle":"2025-03-17T12:29:01.712863Z","shell.execute_reply.started":"2025-03-17T12:29:01.685823Z","shell.execute_reply":"2025-03-17T12:29:01.712278Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2.3 Data imputation for columns with missing values","metadata":{}},{"cell_type":"markdown","source":"Let's check which columns with missing value are we dealing with in the train set","metadata":{}},{"cell_type":"code","source":"missing_percentage = df_train.isnull().mean() * 100 #finds percentage missing in a single column and stores as series \n\nmissing_percentage = missing_percentage.sort_values(ascending = False ) #sorts missing percentage in descending order\n\ncolumn_data_types = df_train.dtypes[missing_percentage.index] #gathers the data types of columns with missing values\n\n# Combine missing percentage and data types into a DataFrame for display\nmissing_percentage_with_types = pd.DataFrame({\n    'Missing Percentage': missing_percentage,\n    'Data Type': column_data_types\n})\n\nprint(missing_percentage_with_types[missing_percentage > 0]) #display columns with missing data\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T12:29:01.713647Z","iopub.execute_input":"2025-03-17T12:29:01.713872Z","iopub.status.idle":"2025-03-17T12:29:01.724451Z","shell.execute_reply.started":"2025-03-17T12:29:01.713848Z","shell.execute_reply":"2025-03-17T12:29:01.723786Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's check which columns with missing value are we dealing with in the test set","metadata":{}},{"cell_type":"code","source":"# Find percentage of missing values for each column in the test set\nmissing_percentage_test = df_test.isnull().mean() * 100\n\n# Sort missing percentage in descending order\nmissing_percentage_test = missing_percentage_test.sort_values(ascending=False)\n\n# Get the data types of columns with missing values\ncolumn_data_types_test = df_test.dtypes[missing_percentage_test.index]\n\n# Combine missing percentage and data types into a DataFrame for display\nmissing_percentage_with_types_test = pd.DataFrame({\n    'Missing Percentage': missing_percentage_test,\n    'Data Type': column_data_types_test\n})\n\n# Display columns with missing data\nprint(missing_percentage_with_types_test[missing_percentage_test > 0])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T12:29:01.725373Z","iopub.execute_input":"2025-03-17T12:29:01.725659Z","iopub.status.idle":"2025-03-17T12:29:01.744358Z","shell.execute_reply.started":"2025-03-17T12:29:01.725630Z","shell.execute_reply":"2025-03-17T12:29:01.743653Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"For `FireplaceQu` , the fact that 47.26% is missing (in train set) could itself be an important reason (Many houses may not have a Fireplace)\nHence we will imputate the missing rows in this column with a new category \"Missing\"","metadata":{}},{"cell_type":"code","source":"# Add 'Missing' as a category to the 'CategoryColumn'\ndf_train['FireplaceQu'] = df_train['FireplaceQu'].cat.add_categories('Missing')\n\n# Fill missing values in categorical columns with a new category 'Unknown'\ndf_train['FireplaceQu'] = df_train['FireplaceQu'].fillna('Missing')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T12:29:01.745305Z","iopub.execute_input":"2025-03-17T12:29:01.745610Z","iopub.status.idle":"2025-03-17T12:29:01.750342Z","shell.execute_reply.started":"2025-03-17T12:29:01.745580Z","shell.execute_reply":"2025-03-17T12:29:01.749653Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Performing similar operation on test set","metadata":{}},{"cell_type":"code","source":"# Add 'Missing' as a category to the 'CategoryColumn'\ndf_test['FireplaceQu'] = df_test['FireplaceQu'].cat.add_categories('Missing')\n\n# Fill missing values in categorical columns with a new category 'Unknown'\ndf_test['FireplaceQu'] = df_test['FireplaceQu'].fillna('Missing')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T12:29:01.751280Z","iopub.execute_input":"2025-03-17T12:29:01.751587Z","iopub.status.idle":"2025-03-17T12:29:01.761334Z","shell.execute_reply.started":"2025-03-17T12:29:01.751560Z","shell.execute_reply":"2025-03-17T12:29:01.760731Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Checking column `FireplaceQu` in train and test set ","metadata":{}},{"cell_type":"code","source":"df_train['FireplaceQu'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T12:29:01.762249Z","iopub.execute_input":"2025-03-17T12:29:01.762535Z","iopub.status.idle":"2025-03-17T12:29:01.773887Z","shell.execute_reply.started":"2025-03-17T12:29:01.762506Z","shell.execute_reply":"2025-03-17T12:29:01.773249Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test[\"FireplaceQu\"].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T12:29:01.774707Z","iopub.execute_input":"2025-03-17T12:29:01.774947Z","iopub.status.idle":"2025-03-17T12:29:01.790124Z","shell.execute_reply.started":"2025-03-17T12:29:01.774922Z","shell.execute_reply":"2025-03-17T12:29:01.789503Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"For the rest of the categorical columns with missing data we will fill the missing the data with the mode of that column","metadata":{}},{"cell_type":"code","source":"# Separate categorical and numerical missing columns\nmissing_categories = missing_percentage_with_types[\n    (missing_percentage_with_types['Missing Percentage'] > 0) & \n    (missing_percentage_with_types['Data Type'] == \"category\")\n].index.tolist()\n\n# Fill missing values\ndf_train[missing_categories] = df_train[missing_categories].apply(lambda x: x.fillna(x.mode()[0]))\n\n# Print results\nprint(\"Categorical columns filled with mode:\", missing_categories)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T12:29:01.791007Z","iopub.execute_input":"2025-03-17T12:29:01.791272Z","iopub.status.idle":"2025-03-17T12:29:01.807408Z","shell.execute_reply.started":"2025-03-17T12:29:01.791234Z","shell.execute_reply":"2025-03-17T12:29:01.806667Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Performing similar operation on test set","metadata":{}},{"cell_type":"code","source":"# Separate categorical columns with missing values in the test dataset\nmissing_categories_test = missing_percentage_with_types_test[\n    (missing_percentage_with_types_test['Missing Percentage'] > 0) & \n    (missing_percentage_with_types_test['Data Type'] == \"category\")\n].index.tolist()\n\n# Fill missing values in categorical columns in the test dataset\ndf_test[missing_categories_test] = df_test[missing_categories_test].apply(lambda x: x.fillna(x.mode()[0]))\n\n# Print results for the test dataset\nprint(\"Categorical columns filled with mode in test dataset:\", missing_categories_test)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T12:29:01.808196Z","iopub.execute_input":"2025-03-17T12:29:01.808433Z","iopub.status.idle":"2025-03-17T12:29:01.824500Z","shell.execute_reply.started":"2025-03-17T12:29:01.808408Z","shell.execute_reply":"2025-03-17T12:29:01.823808Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"For the rest of the numerical columns with missing data we will fill the missing the data with the median of that column","metadata":{}},{"cell_type":"code","source":"missing_numericals = missing_percentage_with_types[\n    (missing_percentage_with_types['Missing Percentage'] > 0) & \n    (missing_percentage_with_types['Data Type'] == \"float64\" )].index.tolist()\n\ndf_train[missing_numericals] = df_train[missing_numericals].apply(lambda x: x.fillna(x.median()))\n\n\nprint(\"Numerical columns filled with medianin train dataset :\", missing_numericals)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T12:29:01.825550Z","iopub.execute_input":"2025-03-17T12:29:01.825832Z","iopub.status.idle":"2025-03-17T12:29:01.834657Z","shell.execute_reply.started":"2025-03-17T12:29:01.825803Z","shell.execute_reply":"2025-03-17T12:29:01.833886Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Performing similar operation on test set","metadata":{}},{"cell_type":"code","source":"# Separate numerical columns with missing values in the test dataset\nmissing_numericals_test = missing_percentage_with_types_test[\n    (missing_percentage_with_types_test['Missing Percentage'] > 0) & \n    (missing_percentage_with_types_test['Data Type'] == \"float64\")\n].index.tolist()\n\n# Fill missing values in numerical columns in the test dataset with median\ndf_test[missing_numericals_test] = df_test[missing_numericals_test].apply(lambda x: x.fillna(x.median()))\n\n# Print results for the test dataset\nprint(\"Numerical columns filled with median in test dataset:\", missing_numericals_test)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T12:29:01.835599Z","iopub.execute_input":"2025-03-17T12:29:01.835870Z","iopub.status.idle":"2025-03-17T12:29:01.852386Z","shell.execute_reply.started":"2025-03-17T12:29:01.835841Z","shell.execute_reply":"2025-03-17T12:29:01.851641Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"kiv target encoding for categorical features \nFinally, we accounted for data skewness by applying log transformations to both the target variable (SalePrice) and key numeric features. ","metadata":{}},{"cell_type":"markdown","source":"## 2.4 Analysing correlation of numerical columns and `SalePrice`","metadata":{}},{"cell_type":"markdown","source":"### Visualising correlation and only keeping the columns that have higher than 0.3 correlation","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Select numerical columns\nnum_columns = df_train.select_dtypes(include=['number']).columns.tolist()\n\n# Select the numerical columns data\ngf = df_train[num_columns]\n\n# Calculate the correlation matrix\ncorr = gf.corr()\n\n\n    # Sort the correlation values with 'SalePrice' in descending order\nsorted_corr = corr['SalePrice'].sort_values(ascending=False)\n\n    # Plot the heatmap for correlation with 'SalePrice'\nplt.figure(figsize=(10, 8))\nsns.heatmap(sorted_corr.to_frame(), annot=True, cmap='coolwarm', vmin=-1, vmax=1)\nplt.title('Correlation with SalePrice')\nplt.show()\n\n    # Get the columns with correlation greater than 0.3\nhg_corr = sorted_corr[sorted_corr > 0.3].index.tolist()\n\n    # Remove 'SalePrice' from the list if it's present\nhg_corr.remove('SalePrice') if 'SalePrice' in hg_corr else None\n\n    # Display the list of columns with high correlation in descending order\nprint(\"Columns with high correlation to SalePrice (sorted):\", hg_corr)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T12:29:01.853290Z","iopub.execute_input":"2025-03-17T12:29:01.853549Z","iopub.status.idle":"2025-03-17T12:29:02.336702Z","shell.execute_reply.started":"2025-03-17T12:29:01.853523Z","shell.execute_reply":"2025-03-17T12:29:02.335931Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Keeping numerical columns with corr > 0.3 ","metadata":{}},{"cell_type":"code","source":"# Identify categorical columns\ncat_columns = df_train.select_dtypes(include=['object', 'category']).columns.tolist()\n\n# Combine the high-correlation numerical columns with all categorical columns\ncolumns_to_keep =  ['Id'] + hg_corr + cat_columns + ['SalePrice'] ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T12:29:02.337751Z","iopub.execute_input":"2025-03-17T12:29:02.338003Z","iopub.status.idle":"2025-03-17T12:29:02.343271Z","shell.execute_reply.started":"2025-03-17T12:29:02.337977Z","shell.execute_reply":"2025-03-17T12:29:02.342507Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now ensuring train and test set has same columns (this allows the prediction to work more efficiently)","metadata":{}},{"cell_type":"code","source":"# Create a filtered dataframe with only the selected columns\ndf_train_filtered = df_train[columns_to_keep]\ndf_test = df_test[['Id'] + hg_corr + cat_columns]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T12:29:02.344286Z","iopub.execute_input":"2025-03-17T12:29:02.344642Z","iopub.status.idle":"2025-03-17T12:29:02.378573Z","shell.execute_reply.started":"2025-03-17T12:29:02.344612Z","shell.execute_reply":"2025-03-17T12:29:02.377876Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Cheching the info of the train and test set","metadata":{}},{"cell_type":"code","source":"# Display the filtered dataframe to verify\nprint(df_train_filtered.info())\nprint(df_test.info())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T12:29:02.379710Z","iopub.execute_input":"2025-03-17T12:29:02.380054Z","iopub.status.idle":"2025-03-17T12:29:02.418367Z","shell.execute_reply.started":"2025-03-17T12:29:02.380024Z","shell.execute_reply":"2025-03-17T12:29:02.417739Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3. Splitting the dataset","metadata":{}},{"cell_type":"markdown","source":"We will split the dataset.\n\n*Note: We don't need to split that train set into the trditional 80/20 split because we will be using cross validation with 5-folds*","metadata":{}},{"cell_type":"code","source":"# Define features (X) and target variable (y)\nX_train = df_train_filtered.drop(columns=['SalePrice'])  # Drop target variable from features\ny_train = df_train_filtered['SalePrice']  # Target variable\n\nX_test = df_test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T12:29:02.419299Z","iopub.execute_input":"2025-03-17T12:29:02.419556Z","iopub.status.idle":"2025-03-17T12:29:02.425146Z","shell.execute_reply.started":"2025-03-17T12:29:02.419531Z","shell.execute_reply":"2025-03-17T12:29:02.424498Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\ndef target_encode(train, test, y_train, categorical_cols):\n    \"\"\"\n    Applies Target Encoding manually using Pandas.\n    \n    Parameters:\n    train (DataFrame): Training feature set.\n    test (DataFrame): Test set (only transformed, not fitted).\n    y_train (Series): Target variable for training.\n    categorical_cols (list): List of categorical column names.\n\n    Returns:\n    DataFrame, DataFrame: Encoded train and test data.\n    \"\"\"\n    train_encoded = train.copy()\n    test_encoded = test.copy()\n    \n    for col in categorical_cols:\n        # Compute mean target value for each category\n        target_means = train.groupby(col)[y_train.name].mean()\n\n        # Map the means to the categorical column in training and test sets\n        train_encoded[col] = train[col].map(target_means)\n        test_encoded[col] = test[col].map(target_means)\n\n        # If unseen categories appear in test, replace them with the overall mean\n        overall_mean = y_train.mean()\n        train_encoded[col].fillna(overall_mean, inplace=True)\n        test_encoded[col].fillna(overall_mean, inplace=True)\n\n    return train_encoded, test_encoded\n\n# Define features (X) and target variable (y)\nX_train = df_train_filtered.drop(columns=['SalePrice'])  # Drop target variable from features\ny_train = df_train_filtered['SalePrice']  # Target variable\nX_test = df_test  # Kaggle test set\n\n# Identify categorical columns automatically (if not specified manually)\ncategorical_cols = X_train.select_dtypes(include=['object']).columns.tolist()\n\n# Apply target encoding\nX_train_encoded, X_test_encoded = target_encode(X_train, X_test, y_train, categorical_cols)\n\n# Now, X_train_encoded is ready for cross-validation (cv=5)\nprint(\"âœ… Target Encoding Completed!\")\nprint(X_train_encoded.head())  # Show sample encoded data\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T12:29:02.426148Z","iopub.execute_input":"2025-03-17T12:29:02.426434Z","iopub.status.idle":"2025-03-17T12:29:02.449291Z","shell.execute_reply.started":"2025-03-17T12:29:02.426405Z","shell.execute_reply":"2025-03-17T12:29:02.448671Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4. Model training and optimazation","metadata":{}},{"cell_type":"markdown","source":"## 4.1 XGBoost and Optuna for hyperparamter tuning","metadata":{}},{"cell_type":"code","source":"!pip install optuna\n!pip install xgboost","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T12:29:02.450122Z","iopub.execute_input":"2025-03-17T12:29:02.450402Z","iopub.status.idle":"2025-03-17T12:29:27.390232Z","shell.execute_reply.started":"2025-03-17T12:29:02.450373Z","shell.execute_reply":"2025-03-17T12:29:27.389326Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import optuna\n\nimport xgboost as xgb\n\n# Patch the issue\ndef _sklearn_tags(self):\n    return {\"estimator_type\": \"regressor\"}\n\nxgb.XGBRegressor.__sklearn_tags__ = _sklearn_tags\n\nfrom sklearn.model_selection import cross_val_score\n\n# Define the objective function\ndef objective(trial):\n    # Define hyperparameter search space\n    params = {\n        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 15),\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000, step=50),\n        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0, 10),\n        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0, 10),\n    }\n    \n    # Initialize the model\n    model = xgb.XGBRegressor(**params,enable_categorical=True, random_state=42)\n\n    \n    # Perform cross-validation\n    score = cross_val_score(model, X_train_encoded, y_train, cv=5, n_jobs=-1)\n    accuracy = score.mean()\n    \n    return accuracy  # Optuna tries to maximize, so return RMSE as negative\n\n# Create study and optimize\nstudy = optuna.create_study(direction=\"maximize\")  # Maximize accuracy\nstudy.optimize(objective, n_trials=20)\n\n# Print best parameters\nprint(\"Best parameters:\", study.best_params)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T12:29:27.391441Z","iopub.execute_input":"2025-03-17T12:29:27.391717Z","iopub.status.idle":"2025-03-17T12:30:07.248514Z","shell.execute_reply.started":"2025-03-17T12:29:27.391689Z","shell.execute_reply":"2025-03-17T12:30:07.247860Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ntrial = study.best_trial\nprint('Accuracy: {}'.format(trial.value))\nprint(\"Best hyperparameters: {}\".format(trial.params))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T12:33:59.231344Z","iopub.execute_input":"2025-03-17T12:33:59.231933Z","iopub.status.idle":"2025-03-17T12:33:59.236116Z","shell.execute_reply.started":"2025-03-17T12:33:59.231898Z","shell.execute_reply":"2025-03-17T12:33:59.235497Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_test.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T12:34:01.169429Z","iopub.execute_input":"2025-03-17T12:34:01.169770Z","iopub.status.idle":"2025-03-17T12:34:01.190174Z","shell.execute_reply.started":"2025-03-17T12:34:01.169741Z","shell.execute_reply":"2025-03-17T12:34:01.189505Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Make predictions using the best model\nbest_params = study.best_params\nbest_model = xgb.XGBRegressor(**best_params, random_state=42,enable_categorical=True)\nbest_model.fit(X_train, y_train)\n\n\n# Predict using the best model\npredictions_log = best_model.predict(X_test)\n\n# Apply inverse log transformation (exponential)\npredictions = np.exp(predictions_log)\n\n# Prepare submission file\nsubmission = pd.DataFrame({\"Id\": df_test[\"Id\"], \"SalePrice\": predictions})\n\nsubmission.head()\n\n# Save to CSV (without index)\nsubmission.to_csv(\"submission.csv\", index=False)\n\nprint(\"Submission file saved as submission.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T12:35:27.236825Z","iopub.execute_input":"2025-03-17T12:35:27.237673Z","iopub.status.idle":"2025-03-17T12:35:32.368670Z","shell.execute_reply.started":"2025-03-17T12:35:27.237635Z","shell.execute_reply":"2025-03-17T12:35:32.367688Z"}},"outputs":[],"execution_count":null}]}